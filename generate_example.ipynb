{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_config\n",
    "from model import CDGPT\n",
    "from tokenizer import SentencePieceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"checkpoints/tokenizer.model\"\n",
    "cfg = get_config()\n",
    "cfg.tokenizer.path = tokenizer_path\n",
    "tokenizer = SentencePieceTokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "fasta_file = \"example.fasta\"\n",
    "parser = SeqIO.parse(fasta_file, \"fasta\")\n",
    "dna_record = next(parser)\n",
    "protein_record = next(parser)\n",
    "reverse_translate_record = next(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"checkpoints/CD-GPT-1b.pth\"\n",
    "state = torch.load(model_path, map_location=\"cpu\")\n",
    "model = CDGPT(cfg)\n",
    "model.load_state_dict(state[\"model\"], strict=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.half().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"<mRNA>{str(dna_record.seq)}</mRNA><translate><:>\"\n",
    "x = tokenizer.encode(prompt, eos=False, device=device)\n",
    "output = model.generate(x,\n",
    "                        max_new_tokens=128,\n",
    "                        temperature=0.8,\n",
    "                        top_k=128,\n",
    "                        top_p=0.0,\n",
    "                        stop_ids=(tokenizer.bos, tokenizer.eos, tokenizer.pad)\n",
    "                        )\n",
    "output = tokenizer.decode(output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[len(prompt):]\n",
    "translate_res = output.split(\"</protein>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_gt = str(dna_record.seq.translate())\n",
    "print(f\"GROUND TRUTH ABOVE, GENERATION BELOW, MISMATCHES IN \\033[91mRED\\033[0m\")\n",
    "print(translate_gt)\n",
    "for i in range(len(translate_gt)):\n",
    "    if translate_res[i] == translate_gt[i]:\n",
    "        print(translate_res[i], end=\"\")\n",
    "    else:\n",
    "        print(f\"\\033[91m{translate_res[i]}\\033[0m\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse translation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download this model from Tencent Weiyun Disk.\n",
    "model_path = \"checkpoints/CD-GPT-1b-reverse-translation.pth\"\n",
    "state = torch.load(model_path, map_location=\"cpu\")\n",
    "model = CDGPT(cfg)\n",
    "model.load_state_dict(state[\"model\"], strict=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.half().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"<protein>{str(protein_record.seq)}</protein><reverse_translate><:>\"\n",
    "x = tokenizer.encode(prompt, eos=False, device=device)\n",
    "output = model.generate(x,\n",
    "                        max_new_tokens=1024,\n",
    "                        temperature=0.8,\n",
    "                        top_k=128,\n",
    "                        top_p=0.0,\n",
    "                        stop_ids=(tokenizer.bos, tokenizer.eos, tokenizer.pad)\n",
    "                        )\n",
    "output = tokenizer.decode(output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[len(prompt):]\n",
    "reverse_translate_res = output.split(\"</mRNA>\")[0].split(\"<mRNA>\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_translate_gt = str(reverse_translate_record.seq)\n",
    "print(f\"GROUND TRUTH ABOVE, GENERATION BELOW, MISMATCHES IN \\033[91mRED\\033[0m\")\n",
    "print(reverse_translate_gt)\n",
    "for i in range(len(reverse_translate_gt)):\n",
    "    if i >= len(reverse_translate_res):\n",
    "        break\n",
    "    if reverse_translate_res[i] == reverse_translate_gt[i]:\n",
    "        print(reverse_translate_res[i], end=\"\")\n",
    "    else:\n",
    "        print(f\"\\033[91m{reverse_translate_res[i]}\\033[0m\", end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
